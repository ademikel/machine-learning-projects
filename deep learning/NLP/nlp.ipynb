{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6644fef-d4f5-463d-a502-d16d2f709d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04054106-1822-40a9-b7a1-b2fb161a8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # downloading relevant dependencies\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011889df-788a-4d4a-975b-e83480878d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "dataset = \"Hello Evreyone. Welcome to this course. We are studying NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee39a64f-7cde-4721-bddb-856459957f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093aa6c1-7c79-49a4-9fe2-3d69ba9cfcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Evreyone.', 'Welcome to this course.', 'We are studying NLP.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing Sentences\n",
    "sent_tokenize(text = dataset, language = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31d704e-32a0-400e-900b-efc900cef9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Evreyone',\n",
       " '.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'this',\n",
       " 'course',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'studying',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing words\n",
    "word_tokenize(text = dataset, language = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd788e18-7128-4cbf-876a-43c7ad696bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Hello Evreyone. Welcome to this course. We are studying NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826c7038-c9d9-4d4c-a0b6-caef32313e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Evreyone',\n",
       " '.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'this',\n",
       " 'course',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'studying',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text = dataset, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe84ac8-b780-4471-b634-c574347013f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5349f724-8c09-4efb-a696-cf8fb001d9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It feels very special when you are loving someone.\\n              We care for our loved ones.\\n              specially when we love each other unconditionally.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ['love', 'loving', 'lover', 'loved', 'lovingly']\n",
    "new_data = \"\"\"It feels very special when you are loving someone.\n",
    "              We care for our loved ones.\n",
    "              specially when we love each other unconditionally.\"\"\"\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c8ff69-333c-4984-81cd-7929e9b1b3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "love\n",
      "lover\n",
      "love\n",
      "lovingli\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "for i in dataset:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63a916c-5882-4776-8a15-b9307748ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "feel\n",
      "veri\n",
      "special\n",
      "when\n",
      "you\n",
      "are\n",
      "love\n",
      "someon\n",
      ".\n",
      "we\n",
      "care\n",
      "for\n",
      "our\n",
      "love\n",
      "one\n",
      ".\n",
      "special\n",
      "when\n",
      "we\n",
      "love\n",
      "each\n",
      "other\n",
      "uncondit\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "word = word_tokenize(text = new_data)\n",
    "for i in word:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59031196-2a62-4952-9444-725211e8cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ec4f99-f3f2-4605-a218-eda7f647095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23ac44db-4bf1-49a2-b9b2-fd597e7b880b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church\n",
      "foot\n",
      "lot\n",
      "gave\n",
      "sat\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "dataset = ['churches', 'feet', 'lot', 'gave', 'sat', 'dogs']\n",
    "for i in dataset:\n",
    "    print(wnl.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c6ae1e5-5f5c-42fb-a387-ee5956f1191d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('better', pos = 'a')\n",
    "# pos is part os speech and 'a' is adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc58411d-7906-4ac7-a084-77f9e3f977d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9c45129-3f80-4f94-a349-6d790990b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"\"\"Hello Mr. Watson, how are you doing today?\n",
    "         The weather is awesome. The garden is Green.\n",
    "         We should go out for a walk.\"\"\"\n",
    "dataset = dataset.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b0ba5e9-e22e-436e-aee1-1c72ecba347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43bc5610-a93f-4ec4-8ed8-e5739467e600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'mr.',\n",
       " 'watson',\n",
       " ',',\n",
       " 'today',\n",
       " '?',\n",
       " 'weather',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'garden',\n",
       " 'green',\n",
       " '.',\n",
       " 'go',\n",
       " 'walk',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentences = []\n",
    "\n",
    "word_tokenize = word_tokenize(text = dataset)\n",
    "\n",
    "for word in word_tokenize:\n",
    "    if word not in stop_words:\n",
    "        filtered_sentences.append(word)\n",
    "        \n",
    "filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a8ed1c-1d6b-4825-8556-7613904253ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of Speech\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48cc17e0-b5bc-41a4-934c-934e98344ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Taj Mahal is one of the world's most celebrated structures\n",
    "             in the world.\n",
    "             It is a syunning symbol of the indian rich history\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3548d00-f4db-4d59-b509-52b50a1b40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing words\n",
    "word = word_tokenize(text = data, language = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9a6cff9-d7b4-4f4c-96c8-83c5589b8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying POS Tagging\n",
    "pos = pos_tag(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b2bdf64-61c1-41d5-912e-91a33329c50d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.help' has no attribute 'upenn_tagsets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1b59023f906c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupenn_tagsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk.help' has no attribute 'upenn_tagsets'"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41f3140-a91a-451f-a218-da4f94474856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "from nltk.chunk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c75193-2866-4012-956f-3d9daecf5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_chunk = \"\"\"\n",
    "chunk:\n",
    "    {<NNPS>+}\n",
    "    {<NNP>+}\n",
    "    {<NN>+}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "248118be-22e7-471f-89c5-3afd2369b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (chunk Taj/NNP Mahal/NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (chunk world/NN)\n",
      "  's/POS\n",
      "  most/RBS\n",
      "  celebrated/JJ\n",
      "  structures/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (chunk world/NN)\n",
      "  ./.\n",
      "  It/PRP\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  syunning/JJ\n",
      "  (chunk symbol/NN)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  indian/JJ\n",
      "  rich/JJ\n",
      "  (chunk history/NN))\n"
     ]
    }
   ],
   "source": [
    "# creating object \n",
    "re = RegexpParser(sequence_chunk)\n",
    "chunked_result = re.parse(pos)\n",
    "print(chunked_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea11c54-8da7-4866-9928-c61d6a9fc264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
